{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度神经网络\n",
    "\n",
    "前面提到的都是浅层神经网络。接下来更深入一些学习深度神经网络。\n",
    "\n",
    "使用DNN会遇到一些问题：\n",
    "\n",
    "- 梯度弥散 vs. 梯度爆炸\n",
    "- 训练很慢\n",
    "- 拥有几百万参数的模型很容易过拟合训练集\n",
    "\n",
    "问题导向，并提出相应的解决方法，都体现在代码中。绝不空谈。\n",
    "\n",
    "终于来到了深度学习的世界！\n",
    "\n",
    "本章主要内容有：\n",
    "\n",
    "- 梯度问题\n",
    "    - Xavier and He初始化\n",
    "    - 不饱和激活函数\n",
    "    - BN\n",
    "    - 梯度剪切\n",
    "- 重用预训练层\n",
    "    - 重用TF模型\n",
    "    - 重用其他框架模型\n",
    "    - 冻结低层layer\n",
    "    - 缓存冻结层\n",
    "    - Tweaking, Dropping, Replacing高层layer\n",
    "    - Model动物园\n",
    "    - 非监督预训练\n",
    "    - 在辅助任务上预训练\n",
    "    \n",
    "- 更快的Optimizer\n",
    "    - Momentum优化\n",
    "    - Nesterov加速梯度\n",
    "    - AdaGrad\n",
    "    - RMSProp\n",
    "    - Adam优化\n",
    "    - 学习率动态调整\n",
    "- 避免过拟合：正则化\n",
    "    - 早停止\n",
    "    - L1,L2正则化\n",
    "    - Dropout\n",
    "    - Max-Norm正则化\n",
    "    - 数据增强\n",
    "- 实际指导原则\n",
    "- 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度弥散/爆炸问题\n",
    "\n",
    "反向传播算法的工作原理是从输出层到输入层，传播误差的梯度值。一旦算法计算出成本函数对每层参数的梯度，算法接着就会使用这些梯度来更新参数。\n",
    "\n",
    "但是梯度通常会慢慢变小，我们称输出层为high layer,越往输入层，越low.导致的结果就是低层的隐层参数不更新，也就不能收敛到好的结果上来。这就是梯度弥撒问题。\n",
    "\n",
    "与此相反，梯度爆炸就是指，梯度越来越大，这在RNN中更为常见一些。\n",
    "\n",
    "总之，DL面临着梯度不稳定的问题。不同的层有不同的学习速度，也即参数更新的速率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T14:14:28.253780Z",
     "start_time": "2018-03-08T14:14:27.069178Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier and He初始化\n",
    "\n",
    "使用tf.layers.dense(), 这个应该就是早期tf.contrib.layers.fully_connected()这个函数。一个稳定一个不稳定。\n",
    "\n",
    "这个是在矩阵的初始化时选择使用的技巧。真正在TF中用时，直接调用即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T14:35:16.316132Z",
     "start_time": "2018-03-08T14:35:13.185581Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T14:36:44.073622Z",
     "start_time": "2018-03-08T14:36:44.028087Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X,n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name='hidden1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非饱和激活函数\n",
    "\n",
    "- Leaky ReLU\n",
    "- ELU\n",
    "- SELU\n",
    "\n",
    "\n",
    "ReLU并不是完美的，比如它面临着一个问题就是：**dying**.就是说训练过程中，有些神经元die后，只输出0。尤其是在学习率较大的情况下更为严重。一旦开始输出0后，就再也回不来了。这是ReLU激活函数的特征。\n",
    "\n",
    "但是实际上，这种情况极为少见。\n",
    "\n",
    "到目前为止，本章的内容我都在论文上有学习过，所以不再继续看本章。我准备先看13章，CNN部分，然后RNN，强化学习，最后看第12章节的分布式TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
